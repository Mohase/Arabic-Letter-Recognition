{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b72377-bbfc-4b7c-8d73-cda331615bd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1><center>Machine Learning applied to Arabic Handwritten Charachters</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369d049-be31-4257-8010-9f5c2b63806d",
   "metadata": {},
   "source": [
    "\\setcounter{secnumdepth}{0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d3fdc-4504-446e-b693-a88d6b1f0f69",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p1><center>September 2024</center></p1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a23b3-b5bc-4f51-86aa-8f48d9c5b6f2",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704a33e-2d15-4303-aa2b-5409e3df4b2e",
   "metadata": {},
   "source": [
    "The task of Arabic handwritten character classification involves categorizing handwritten characters into distinct classes based on their morphology. The complexity arises due to variations in individual writing styles and the cursive nature of the Arabic script. The ability to accurately classify Arabic handwritten characters is crucial for enhancing various applications, including Arabic Optical Character Recognition (OCR) systems and digital archiving. \n",
    "\n",
    "In this study, two machine learning methods—Logistic Regression [1] and Support Vector Classifier (SVC) [2]—are compared using accuracy, precision, recall, and F1-score as performance metrics. Additionally, k-fold cross-validation [3] is applied to ensure robustness across different data splits. We employ Principal Component Analysis (PCA) [4] for dimensionality reduction on the Arabic Handwritten Characters Dataset (AHCD) [5] to improve the accuracy of the models.\n",
    "\n",
    "The report is structured as follows: Section 2 outlines the problem formulation, including data types, sources, and labels. Section 3 discusses the methodology, covering the hypothesis space, preprocessing steps, dataset splitting, and feature engineering. Section 4 presents empirical results comparing the methods, while Section 5 concludes with an analysis of the results and future directions. The report will conclude with code used for this task as a appendix, which will also feature the findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682099a0-33bb-4aff-b240-39546e7048ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Problem Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7529ee-0c4a-4075-a5a2-0e247eb5b276",
   "metadata": {},
   "source": [
    "The AHCD dataset contains high-dimensional inputs, with each instance being a grayscale image represented as a 1x1024 feature vector, where each element corresponds to a pixel value ranging from 0 (black) to 255 (white). The dataset contains 16,800 images of handwritten Arabic characters, categorized into 28 distinct classes, representing individual letters of the Arabic alphabet.\n",
    "\n",
    "This is a supervised learning task, where each image is associated with a label indicating the correct Arabic character. The goal of this study is to develop a machine learning model capable of accurately predicting the corresponding label for a given handwritten image. By employing dimensionality reduction techniques like PCA, we aim to extract the most significant features from the images to improve classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355353b8-e38c-469c-b338-f5402ef02ae1",
   "metadata": {},
   "source": [
    "# 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91a596-b573-47f8-99c1-e1386f655cf4",
   "metadata": {},
   "source": [
    "### 3.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd2fcd-30db-4e45-a208-c6925fa7938e",
   "metadata": {},
   "source": [
    "The AHCD dataset [5] contains 16,800 grayscale images of Arabic handwritten characters, publicly available through this link. Each image is resized to 32x32 pixels and then flattened into a 1024-dimensional feature vector. The dataset is split into 75% for training, 10% for validation, and 15% for testing using the train_test_split function from scikit-learn [6].\n",
    "\n",
    "**Training Set:** 75% of the dataset (12,600 images) is used to train the models.\n",
    "\n",
    "**Validation Set:** 10% of the dataset (1,680 images) is reserved for tuning hyperparameters.\n",
    "\n",
    "**Test Set:** 20% of the dataset (3,360 images) is used for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229f100-71bb-4241-80b8-72a01edff09e",
   "metadata": {},
   "source": [
    "### 3.2 Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67341058-edb6-4444-a995-f3926d889a8a",
   "metadata": {},
   "source": [
    "Each 32x32 grayscale image is flattened into a 1x1024 feature vector. Given that processing 1024 features can be computationally expensive and redundant, Principal Component Analysis (PCA) [4] is employed to reduce the feature space from 1024 to 64 components, to retain adequate amount of the variance. This dimensionality reduction helps to focus on the most informative features, improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ad61c-a565-463f-b291-cbbfdae968a4",
   "metadata": {},
   "source": [
    "### 3.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dcea5-803a-4861-91f6-8fdf75f1b525",
   "metadata": {},
   "source": [
    "We apply the Logistic Regression model to classify the images. Logistic regression is chosen due to its simplicity, interpretability, and effectiveness in handling high-dimensional data Support Vector Classifier (SVC) for classification and compare their performance based on accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc4e6a-380a-472e-bac6-fb68808a55ba",
   "metadata": {},
   "source": [
    "#### 3.3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05b060-fac3-43f3-9878-7c17ebc51beb",
   "metadata": {},
   "source": [
    "Logistic Regression is extended to handle the multiclass nature of this task using the one-vs-all approach, where each character class is treated as a binary classification problem. The hypothesis function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f2864-c524-417a-bea7-8592d8d2863a",
   "metadata": {},
   "source": [
    "<center>$h^{(K)}_{θ}(x) = P (y = K|x; θ^{(K)})$                                                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bed66-e664-4825-ad2c-468fec2cdb26",
   "metadata": {},
   "source": [
    "where $h^{(K)}(x)$ represents the probability of class $K$ for input $x$. Logistic Regression's ability to provide probabilistic predictions and its resistance to overfitting make it a suitable choice for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6d18b-0d12-4a9a-a6d9-8757994a26e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Logistic Regression is a strong candidate for recognizing Arabic alphabet characters due to its effectiveness in modeling class probabilities. It establishes a weight matrix \n",
    "$θ^{K}$ that, when combined with the feature vector, generates a weight vector for each character. The predicted character corresponds to the class with the highest value in this vector.\n",
    "\n",
    "The preference for Logistic Regression in Arabic character recognition is due to its reduced likelihood of overfitting and its ability to produce probabilistic outputs. This feature facilitates confidence assessment in predictions, which is especially important when differentiating between similar-looking characters. Moreover, its simplicity in implementation and efficient performance make it a suitable choice for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871f214-1881-443f-bece-8c08b1813ad5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3.2 Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4483a6-5d40-4111-887d-0d531837fcc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Support Vector Classifier (SVC) constructs a hyperplane that maximizes the margin between different classes. For non-linearly separable data, kernel functions are employed to project the input features into higher dimensions. The decision function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5364d12-7abf-4aa3-a9b9-d88b812d04ec",
   "metadata": {},
   "source": [
    "<center>$h(x) = sign (w * x + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73290ada-58d5-43f0-b363-ade9eb59073e",
   "metadata": {},
   "source": [
    "SVC's proficiency in handling high-dimensional data and its effectiveness in maximizing the margin between classes make it a strong candidate for handwritten character classification. In this task, the function $h(x)$ predicts the class label for the input data point $x$. It assigns a labels in the following manner: \n",
    "\n",
    "- $+1$ when $wx+b$ $>=$ 0, and\n",
    "-  $-1$ when $< 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3535245-5bc5-4e71-b23f-46efce956e69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.4 Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6484b8-650d-47da-93f9-617b9fc5162f",
   "metadata": {},
   "source": [
    "We use categorical cross-entropy [7] for Logistic Regression and hinge loss [8] for SVC. Categorical cross-entropy minimizes the dissimilarity between predicted probabilities and true labels, while hinge loss ensures maximum separation between classes by penalizing predictions close to the decision boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02f873-0a25-46b1-b203-b4d5134f410c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4.1 Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29f702-ea97-47c9-87be-a0c6d347bced",
   "metadata": {},
   "source": [
    "Thanks to the LogisticRegression calssifier, Categorical Cross-Entropy Loss is applied automatically. This loss function is effective because it penalizes incorrect predictions, particularly when the model is confident about an incorrect class. The closer the predicted probability is to the true label, the smaller the loss. If the predicted probability of the correct class is low, the loss is large. The following is a suitable CCEL equation when dealing multi-class classification and multiple samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a600868-5ca3-49c7-8b5c-b059976c26b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>$L_{\\text{CE}} = -\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{ic} \\log(p_{ic})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4ada3-922f-4134-b2f8-f8b1275453d9",
   "metadata": {},
   "source": [
    "**Where:**\n",
    "\n",
    "- $N$ is the number of samples,\n",
    "- $C$ is the number of classes,\n",
    "- $y_{ic}$ is the binary indicator (0 or 1) if class $c$ is the correct label for sample $i$,\n",
    "- $p_{ic}$ is the predicted probability that sample $i$ belongs to class $c$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca40720-8290-495d-8675-4073bd797fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.4.2 Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf0dc7-ad93-4243-b0a7-5eb6644c08aa",
   "metadata": {},
   "source": [
    "Hinge Loss is commonly used with Support Vector Machines (SVMs) to encourage margin-maximizing hyperplanes for effective class separation. It penalizes predictions close to the decision boundary and heavily penalizes misclassifications, making it computationally efficient and robust for training SVC models. The loss function is defined as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327a2df-ff39-47e0-82b0-6be741f6ddc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>$L(y, f(x)) = \\max(0, 1 - y \\cdot f(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa7e02-721e-4963-aba3-500d49f94dc2",
   "metadata": {},
   "source": [
    "**Where:**\n",
    "- $y$ is the true class label $(+1 or -1)$,\n",
    "- $f(x)$ is the decision function of the classifier for a given data point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c8702-ffb9-49c4-8c1f-e7b57b419adf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444b1f3-0d82-442a-9fd4-d2e9329cbf4d",
   "metadata": {},
   "source": [
    "#### 4.1 Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c43aa-6781-450f-a6c3-15b0a6172144",
   "metadata": {},
   "source": [
    "Our models (/methods) are evaluated using accuracy, precision, recall, and F1-score. The following is a brief explanation of each model evaluation method with their respective equations:\n",
    "\n",
    "**Accuracy:** Measures the proportion of correct predictions.\n",
    "\n",
    "<center>$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "\n",
    "**Precision:** Evaluates the accuracy of positive class predictions.\n",
    "    \n",
    "\n",
    "<center>$Precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "**Recall:** Measures the ability to identify all relevant instances.\n",
    "    \n",
    "\n",
    "<center>$Precision = \\frac{TP}{TP + FN}$\n",
    "\n",
    "    \n",
    "**F1-score:** Provides a harmonic mean of precision and recall (i.e the overall \"health\" of the model).\n",
    "    \n",
    "    \n",
    "<center>$F1-score = 2 * \\frac{Precision * Recall}{Precision + Recall}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867bd3d-9b63-41ca-981f-6fd517e5dadd",
   "metadata": {},
   "source": [
    "**Where:**\n",
    " - $TP$: True positive,\n",
    " - $FP$: False positive,\n",
    " - $TN$: True negative,\n",
    " - $FN$: False positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645ec47-0eb2-4696-b8fd-b1dc12ffe154",
   "metadata": {},
   "source": [
    "#### 4.2 Empirical Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed6de1-2c63-44cd-90f4-38bd442e6a91",
   "metadata": {},
   "source": [
    "(Findings below appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dd322-8c54-4bfd-b9d6-c7026a9f3073",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4.2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0260442-3a5c-4731-b81e-84a44cbf297a",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved an overall accuracy of 45.84% on the training set, with a corresponding error of 0.5416. On the validation set, the model reached an accuracy of 39.83%, with a higher error of 0.6017. This suggests that Logistic Regression slightly overfits the training data, as the validation accuracy is lower than the training accuracy. The model’s inability to generalize well to unseen data is further evidenced by its test set accuracy of 40.80%, resulting in a test error of 0.5920.\n",
    "\n",
    "The difference between training, validation, and test accuracies highlights Logistic Regression’s limitations in capturing the complexity of this high-dimensional dataset, which requires more than just linear decision boundaries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d3ca3-6f27-4a92-a1bd-6020e0581cef",
   "metadata": {},
   "source": [
    "##### 4.2.2 Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf5257-66fa-496d-be34-6f42217debd7",
   "metadata": {},
   "source": [
    "The Support Vector Classifier (SVC) demonstrated much better overall performance compared to Logistic Regression. The model achieved an accuracy of 69.17% on the training set, resulting in a training error of 0.3083. On the validation set, SVC achieved an accuracy of 54.91%, with a validation error of 0.4509, which indicates some degree of overfitting, though less pronounced than in Logistic Regression.\n",
    "\n",
    "SVC’s performance on the test set was also higher, with an accuracy of 57.20%, resulting in a test error of 0.4280. This suggests that SVC was able to capture more complex decision boundaries, though there is still room for improvement in generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfbf4db-3573-402c-883d-c82c0917307c",
   "metadata": {},
   "source": [
    "##### 4.3 Summary of the results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a201-e114-470e-a3b1-72280138b909",
   "metadata": {},
   "source": [
    "In comparing the training, validation, and test errors of Logistic Regression and SVC, it is clear that SVC outperformed Logistic Regression across all sets. Logistic Regression exhibited both lower training and validation accuracy, and struggled to generalize to the test data. SVC, while showing better performance overall, still experienced a drop in accuracy between the training and validation sets, indicative of some overfitting.\n",
    "\n",
    "The training, validation, and test errors for both models can be summarized as follows:\n",
    "\n",
    "| Model                | Training Accuracy | Validation Accuracy | Test Accuracy | Training Error | Validation Error | Test Error  |\n",
    "|----------------------|-------------------|---------------------|---------------|----------------|------------------|-------------|\n",
    "| Logistic Regression   | 45.84%            | 39.83%              | 40.80%        | 0.5416         | 0.6017           | 0.5920      |\n",
    "| SVC                   | 69.17%            | 54.91%              | 57.20%        | 0.3083         | 0.4509           | 0.4280      |\n",
    "\n",
    "The SVC model was selected as the final method due to its superior accuracy on both the validation and test sets, as well as its lower test error of **0.4280** compared to Logistic Regression's **0.5920**. However, further improvements can be achieved by using more advanced techniques, such as Convolutional Neural Networks (CNNs), which are well-suited for image data and could likely outperform both Logistic Regression and SVC in this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7becebb-4a02-46c3-9ee2-e5ff622da98a",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6262e5-3206-47b0-86c8-2b932389b968",
   "metadata": {
    "tags": []
   },
   "source": [
    "his study compared Logistic Regression and Support Vector Classifier (SVC) for a multi-class classification task. The SVC outperformed Logistic Regression, achieving a test accuracy of 57.20% compared to 40.80%. The SVC's ability to handle non-linear boundaries, made it the better choice, whereas Logistic Regression struggled with the complexity of the dataset.\n",
    "\n",
    "However, the performance of SVC, while better, is still limited, with the model showing signs of overfitting and relatively low generalization. Neither method achieved satisfactory accuracy, suggesting that the problem remains unsolved.\n",
    "\n",
    "\n",
    "To improve results further, similar tasks should focus on applying Convolutional Neural Networks (CNNs), which are better suited for image data. Data augmentation and further hyperparameter tuning could also help improve the models' generalization and performance.\n",
    "\n",
    "In conclusion, SVC was the better model in this comparison, but exploring more advanced techniques like CNNs is recommended for better accuracy. (Examples of CNN application to the same data can be seen in the source link of the data [5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c47bea-4114-43b8-8d0d-8e86dbf7803e",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571042ad-af62-4fcc-8638-398358477ad4",
   "metadata": {},
   "source": [
    "\n",
    "1. Raymond E Wright. Logistic regression. 1995.\n",
    "\n",
    "2. William S Noble. What is a support vector machine? Nature biotechnology, 24(12):1565–1567, 2006.\n",
    "\n",
    "3. Tadayoshi Fushiki. Estimation of prediction error by using k-fold cross-validation. Statistics and Computing, 21:137–146, 2011.\n",
    "\n",
    "4. I.T. Jolliffe. Principal component analysis. Springer Verlag, New York, 1986.\n",
    "\n",
    "5. Arabic Handwritten Characters Dataset. Kaggle, 2021. https://www.kaggle.com/datasets/mloey1/ahcd1?select=csvTrainLabel+13440x1.csvhttps://www.kaggle.com/datasets/ahmedalafeef/arabic-handwritten-characters-dataset\n",
    "\n",
    "6. Scikit-learn Documentation. Train-test split. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "7. Categorical Cross-Entropy Loss: https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n",
    "\n",
    "8. Hinge Loss for SVM: https://en.wikipedia.org/wiki/Hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d50f628-38cd-404b-a421-e4808c9fc5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Training Set Results:\n",
      "Results for Logistic Regression (Train):\n",
      "Accuracy: 0.4584\n",
      "Precision: 0.4623\n",
      "Recall: 0.4586\n",
      "F1-Score: 0.4553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.89      0.86       397\n",
      "           2       0.47      0.63      0.53       411\n",
      "           3       0.36      0.33      0.35       412\n",
      "           4       0.46      0.44      0.45       414\n",
      "           5       0.42      0.33      0.37       403\n",
      "           6       0.38      0.33      0.35       415\n",
      "           7       0.46      0.24      0.32       406\n",
      "           8       0.40      0.61      0.48       404\n",
      "           9       0.44      0.41      0.43       405\n",
      "          10       0.48      0.61      0.54       415\n",
      "          11       0.57      0.56      0.57       424\n",
      "          12       0.41      0.50      0.45       393\n",
      "          13       0.61      0.41      0.49       403\n",
      "          14       0.47      0.42      0.45       395\n",
      "          15       0.49      0.45      0.47       411\n",
      "          16       0.40      0.45      0.43       401\n",
      "          17       0.45      0.39      0.42       408\n",
      "          18       0.41      0.40      0.41       420\n",
      "          19       0.51      0.55      0.53       419\n",
      "          20       0.42      0.45      0.44       415\n",
      "          21       0.45      0.35      0.39       401\n",
      "          22       0.55      0.48      0.51       395\n",
      "          23       0.50      0.50      0.50       415\n",
      "          24       0.36      0.52      0.43       401\n",
      "          25       0.34      0.36      0.35       410\n",
      "          26       0.52      0.50      0.50       410\n",
      "          27       0.35      0.38      0.37       408\n",
      "          28       0.45      0.34      0.39       413\n",
      "\n",
      "    accuracy                           0.46     11424\n",
      "   macro avg       0.46      0.46      0.46     11424\n",
      "weighted avg       0.46      0.46      0.46     11424\n",
      "\n",
      "Validation Set Results:\n",
      "Results for Logistic Regression (Val):\n",
      "Accuracy: 0.3983\n",
      "Precision: 0.3944\n",
      "Recall: 0.3958\n",
      "F1-Score: 0.3905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.76      0.75        83\n",
      "           2       0.46      0.62      0.53        69\n",
      "           3       0.25      0.25      0.25        68\n",
      "           4       0.34      0.36      0.35        66\n",
      "           5       0.43      0.34      0.38        77\n",
      "           6       0.20      0.18      0.19        65\n",
      "           7       0.23      0.11      0.15        74\n",
      "           8       0.37      0.50      0.42        76\n",
      "           9       0.43      0.41      0.42        75\n",
      "          10       0.43      0.54      0.48        65\n",
      "          11       0.39      0.46      0.42        56\n",
      "          12       0.44      0.44      0.44        87\n",
      "          13       0.44      0.27      0.34        77\n",
      "          14       0.51      0.44      0.47        85\n",
      "          15       0.39      0.46      0.42        69\n",
      "          16       0.38      0.41      0.39        79\n",
      "          17       0.37      0.31      0.34        72\n",
      "          18       0.31      0.30      0.30        60\n",
      "          19       0.37      0.41      0.39        61\n",
      "          20       0.32      0.38      0.35        65\n",
      "          21       0.41      0.33      0.37        79\n",
      "          22       0.45      0.39      0.42        85\n",
      "          23       0.39      0.42      0.40        65\n",
      "          24       0.38      0.51      0.43        79\n",
      "          25       0.29      0.37      0.32        70\n",
      "          26       0.63      0.47      0.54        70\n",
      "          27       0.36      0.36      0.36        72\n",
      "          28       0.33      0.28      0.31        67\n",
      "\n",
      "    accuracy                           0.40      2016\n",
      "   macro avg       0.39      0.40      0.39      2016\n",
      "weighted avg       0.40      0.40      0.39      2016\n",
      "\n",
      "\n",
      "--- Support Vector Classifier (SVC) ---\n",
      "Training Set Results:\n",
      "Results for SVC (Train):\n",
      "Accuracy: 0.6917\n",
      "Precision: 0.7120\n",
      "Recall: 0.6920\n",
      "F1-Score: 0.6959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.92      0.91       397\n",
      "           2       0.66      0.77      0.71       411\n",
      "           3       0.55      0.58      0.56       412\n",
      "           4       0.74      0.55      0.63       414\n",
      "           5       0.62      0.70      0.66       403\n",
      "           6       0.67      0.58      0.62       415\n",
      "           7       0.74      0.55      0.63       406\n",
      "           8       0.36      0.76      0.49       404\n",
      "           9       0.55      0.43      0.49       405\n",
      "          10       0.57      0.65      0.61       415\n",
      "          11       0.67      0.62      0.64       424\n",
      "          12       0.64      0.67      0.66       393\n",
      "          13       0.87      0.74      0.80       403\n",
      "          14       0.78      0.63      0.70       395\n",
      "          15       0.70      0.72      0.71       411\n",
      "          16       0.65      0.71      0.68       401\n",
      "          17       0.80      0.63      0.71       408\n",
      "          18       0.71      0.76      0.73       420\n",
      "          19       0.82      0.77      0.79       419\n",
      "          20       0.64      0.68      0.66       415\n",
      "          21       0.76      0.63      0.69       401\n",
      "          22       0.85      0.79      0.82       395\n",
      "          23       0.88      0.77      0.82       415\n",
      "          24       0.73      0.84      0.78       401\n",
      "          25       0.61      0.64      0.62       410\n",
      "          26       0.85      0.82      0.83       410\n",
      "          27       0.76      0.73      0.74       408\n",
      "          28       0.85      0.72      0.78       413\n",
      "\n",
      "    accuracy                           0.69     11424\n",
      "   macro avg       0.71      0.69      0.70     11424\n",
      "weighted avg       0.71      0.69      0.70     11424\n",
      "\n",
      "Validation Set Results:\n",
      "Results for SVC (Val):\n",
      "Accuracy: 0.5491\n",
      "Precision: 0.5663\n",
      "Recall: 0.5466\n",
      "F1-Score: 0.5460\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.87      0.88        83\n",
      "           2       0.58      0.70      0.63        69\n",
      "           3       0.45      0.50      0.47        68\n",
      "           4       0.55      0.35      0.43        66\n",
      "           5       0.46      0.49      0.47        77\n",
      "           6       0.34      0.29      0.31        65\n",
      "           7       0.53      0.24      0.33        74\n",
      "           8       0.34      0.70      0.45        76\n",
      "           9       0.50      0.39      0.44        75\n",
      "          10       0.46      0.54      0.50        65\n",
      "          11       0.49      0.46      0.48        56\n",
      "          12       0.56      0.55      0.56        87\n",
      "          13       0.63      0.52      0.57        77\n",
      "          14       0.58      0.42      0.49        85\n",
      "          15       0.48      0.62      0.54        69\n",
      "          16       0.61      0.53      0.57        79\n",
      "          17       0.59      0.51      0.55        72\n",
      "          18       0.43      0.50      0.47        60\n",
      "          19       0.34      0.64      0.44        61\n",
      "          20       0.52      0.52      0.52        65\n",
      "          21       0.67      0.54      0.60        79\n",
      "          22       0.79      0.59      0.68        85\n",
      "          23       0.82      0.75      0.78        65\n",
      "          24       0.67      0.77      0.72        79\n",
      "          25       0.45      0.54      0.49        70\n",
      "          26       0.75      0.64      0.69        70\n",
      "          27       0.77      0.56      0.65        72\n",
      "          28       0.61      0.55      0.58        67\n",
      "\n",
      "    accuracy                           0.55      2016\n",
      "   macro avg       0.57      0.55      0.55      2016\n",
      "weighted avg       0.57      0.55      0.55      2016\n",
      "\n",
      "\n",
      "Logistic Regression Cross-Validation Accuracy Scores:  [0.40087527 0.41619256 0.42800875 0.39781182 0.4119965 ]\n",
      "Logistic Regression Mean Accuracy:  0.41097697999977\n",
      "\n",
      "SVC Cross-Validation Accuracy Scores:  [0.55448578 0.54792123 0.54923414 0.54266958 0.54684764]\n",
      "SVC Mean Accuracy:  0.5482316715654904\n",
      "\n",
      "Final Test Set Evaluation:\n",
      "Results for Logistic Regression (Test):\n",
      "Accuracy: 0.4080\n",
      "Precision: 0.4089\n",
      "Recall: 0.4080\n",
      "F1-Score: 0.4045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.85      0.84       120\n",
      "           2       0.39      0.50      0.44       120\n",
      "           3       0.28      0.27      0.27       120\n",
      "           4       0.36      0.33      0.34       120\n",
      "           5       0.36      0.31      0.33       120\n",
      "           6       0.38      0.25      0.30       120\n",
      "           7       0.36      0.25      0.29       120\n",
      "           8       0.43      0.58      0.50       120\n",
      "           9       0.35      0.41      0.38       120\n",
      "          10       0.48      0.62      0.54       120\n",
      "          11       0.48      0.49      0.48       120\n",
      "          12       0.38      0.39      0.39       120\n",
      "          13       0.59      0.34      0.43       120\n",
      "          14       0.38      0.38      0.38       120\n",
      "          15       0.40      0.40      0.40       120\n",
      "          16       0.34      0.38      0.36       120\n",
      "          17       0.38      0.33      0.35       120\n",
      "          18       0.35      0.34      0.35       120\n",
      "          19       0.51      0.54      0.52       120\n",
      "          20       0.36      0.38      0.37       120\n",
      "          21       0.35      0.30      0.32       120\n",
      "          22       0.50      0.46      0.48       120\n",
      "          23       0.44      0.42      0.43       120\n",
      "          24       0.34      0.48      0.40       120\n",
      "          25       0.34      0.31      0.32       120\n",
      "          26       0.42      0.45      0.43       120\n",
      "          27       0.31      0.32      0.31       120\n",
      "          28       0.38      0.34      0.36       120\n",
      "\n",
      "    accuracy                           0.41      3360\n",
      "   macro avg       0.41      0.41      0.40      3360\n",
      "weighted avg       0.41      0.41      0.40      3360\n",
      "\n",
      "Results for SVC (Test):\n",
      "Accuracy: 0.5720\n",
      "Precision: 0.5899\n",
      "Recall: 0.5720\n",
      "F1-Score: 0.5723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.87      0.83       120\n",
      "           2       0.57      0.65      0.61       120\n",
      "           3       0.42      0.47      0.44       120\n",
      "           4       0.57      0.41      0.48       120\n",
      "           5       0.42      0.52      0.46       120\n",
      "           6       0.52      0.40      0.45       120\n",
      "           7       0.49      0.28      0.36       120\n",
      "           8       0.36      0.77      0.49       120\n",
      "           9       0.54      0.42      0.47       120\n",
      "          10       0.52      0.56      0.54       120\n",
      "          11       0.63      0.52      0.57       120\n",
      "          12       0.57      0.55      0.56       120\n",
      "          13       0.65      0.61      0.63       120\n",
      "          14       0.57      0.44      0.50       120\n",
      "          15       0.52      0.51      0.51       120\n",
      "          16       0.57      0.65      0.61       120\n",
      "          17       0.61      0.47      0.53       120\n",
      "          18       0.56      0.59      0.58       120\n",
      "          19       0.41      0.68      0.51       120\n",
      "          20       0.51      0.58      0.54       120\n",
      "          21       0.59      0.43      0.50       120\n",
      "          22       0.82      0.72      0.77       120\n",
      "          23       0.88      0.72      0.79       120\n",
      "          24       0.64      0.76      0.69       120\n",
      "          25       0.54      0.57      0.56       120\n",
      "          26       0.78      0.69      0.73       120\n",
      "          27       0.75      0.57      0.65       120\n",
      "          28       0.72      0.62      0.66       120\n",
      "\n",
      "    accuracy                           0.57      3360\n",
      "   macro avg       0.59      0.57      0.57      3360\n",
      "weighted avg       0.59      0.57      0.57      3360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from PIL import Image\n",
    "\n",
    "# Loading the data\n",
    "train_labels = pd.read_csv('/notebooks/ml project/csvTrainLabel 13440x1.csv', header=None)\n",
    "train_images = pd.read_csv('/notebooks/ml project/csvTrainImages 13440x1024.csv', header=None)\n",
    "test_labels = pd.read_csv('/notebooks/ml project/csvTestLabel 3360x1.csv', header=None)\n",
    "test_images = pd.read_csv('/notebooks/ml project/csvTestImages 3360x1024.csv', header=None)\n",
    "\n",
    "# Function to reshape images from the flattened format\n",
    "def reshape_images(data, img_size=(32, 32)):\n",
    "    reshaped_images = []\n",
    "    for index, row in data.iterrows():\n",
    "        image_array = np.array(row).reshape(img_size)\n",
    "        pil_image = Image.fromarray(image_array.astype('uint8'), mode='L')\n",
    "        rotated_image = pil_image.rotate(-90, expand=True)\n",
    "        mirrored_image = rotated_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        reshaped_images.append(np.array(mirrored_image).reshape(img_size))\n",
    "    return np.array(reshaped_images)\n",
    "\n",
    "# Calling the function to reshape the train and test images\n",
    "reshaped_train_images = reshape_images(train_images)\n",
    "reshaped_test_images = reshape_images(test_images)\n",
    "\n",
    "# Function to display the first 5 images in a grid (after getting them rotated and mirrored)\n",
    "def display_images(images, num_images=5):\n",
    "    plt.figure(figsize=(10, 5))  \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i+1)  \n",
    "        plt.imshow(images[i], cmap='gray')  \n",
    "        plt.axis('off') \n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "\n",
    "# Displaying the first 5 reshaped images\n",
    "display_images(reshaped_train_images[:5])\n",
    "\n",
    "# Function to flatten images from the reshaped format into 1 x 1024 vectors\n",
    "def flatten_images(data):\n",
    "    flattened_images = [image.flatten() for image in data]\n",
    "    return np.array(flattened_images)\n",
    "\n",
    "# Calling the function to flatten the reshaped images\n",
    "flattened_train_images = flatten_images(reshaped_train_images)\n",
    "flattened_test_images = flatten_images(reshaped_test_images)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(flattened_train_images, train_labels, test_size=0.15, random_state=32)\n",
    "\n",
    "# Scale the data before PCA\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(flattened_test_images)\n",
    "\n",
    "# Apply PCA for dimensionality reduction (64)\n",
    "pca = PCA(n_components=64)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Prepare training samples\n",
    "X_train_sample = X_train_pca\n",
    "y_train_sample = y_train.values.ravel()\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_sample, y_train_sample)\n",
    "y_pred_logreg_train = log_reg.predict(X_train_pca)\n",
    "y_pred_logreg_val = log_reg.predict(X_val_pca)\n",
    "\n",
    "# Train Support Vector Classifier (SVC)\n",
    "svc = SVC(C=1.0, kernel='rbf', gamma='scale')\n",
    "svc.fit(X_train_sample, y_train_sample)\n",
    "y_pred_svc_train = svc.predict(X_train_pca)\n",
    "y_pred_svc_val = svc.predict(X_val_pca)\n",
    "\n",
    "# Evaluating Logistic Regression on Training and Validation Sets\n",
    "print(\"\\n--- Logistic Regression ---\")\n",
    "print(\"Training Set Results:\")\n",
    "evaluate_model(y_train, y_pred_logreg_train, \"Logistic Regression (Train)\")\n",
    "print(\"Validation Set Results:\")\n",
    "evaluate_model(y_val, y_pred_logreg_val, \"Logistic Regression (Val)\")\n",
    "\n",
    "# Evaluating SVC on Training and Validation Sets\n",
    "print(\"\\n--- Support Vector Classifier (SVC) ---\")\n",
    "print(\"Training Set Results:\")\n",
    "evaluate_model(y_train, y_pred_svc_train, \"SVC (Train)\")\n",
    "print(\"Validation Set Results:\")\n",
    "evaluate_model(y_val, y_pred_svc_val, \"SVC (Val)\")\n",
    "\n",
    "# Adding KFold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=32)\n",
    "\n",
    "# Logistic Regression Cross-Validation\n",
    "logreg_cv_scores = cross_val_score(log_reg, X_train_pca, y_train_sample, cv=kf, scoring='accuracy')\n",
    "print(\"\\nLogistic Regression Cross-Validation Accuracy Scores: \", logreg_cv_scores)\n",
    "print(\"Logistic Regression Mean Accuracy: \", np.mean(logreg_cv_scores))\n",
    "\n",
    "# SVC Cross-Validation\n",
    "svc_cv_scores = cross_val_score(svc, X_train_pca, y_train_sample, cv=kf, scoring='accuracy')\n",
    "print(\"\\nSVC Cross-Validation Accuracy Scores: \", svc_cv_scores)\n",
    "print(\"SVC Mean Accuracy: \", np.mean(svc_cv_scores))\n",
    "\n",
    "# Final Test Set Evaluation for both models\n",
    "y_pred_logreg_test = log_reg.predict(X_test_pca)\n",
    "y_pred_svc_test = svc.predict(X_test_pca)\n",
    "\n",
    "print(\"\\nFinal Test Set Evaluation:\")\n",
    "evaluate_model(test_labels, y_pred_logreg_test, \"Logistic Regression (Test)\")\n",
    "evaluate_model(test_labels, y_pred_svc_test, \"SVC (Test)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
